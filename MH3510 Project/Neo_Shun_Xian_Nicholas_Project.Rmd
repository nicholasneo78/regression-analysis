---
title: "MH3510 Project Report"
author: "Neo Shun Xian Nicholas"
date: "27 October 2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Background
This data is about traffic monitoring. One of the most important traffic monitoring variables is the average annual daily traffic (aadt) for a section of road or highway. It is defined as the average, over a year, of the the number of vehicles that pass through a particular section of a road each day.  
  
  
**Response Variable:**   
y: Average annual daily traffic 
  
**Predictor Variable:**  
X~1~: Population of country in which road section is located  
X~2~: Number of lanes in road section   
X~3~: Width of the road section (in feet)  
X~4~: Two-category quality variable indicating whether or not there is control of access to road section (1=access control, 2=no access control)  
**Can define X~4~ to be 1 if there is access control and 0 if no control**  
  
*Numerical predictor variable:* X~1~, X~2~, X~3~    
*Categorical predictor varable:* X~4~  

## Importing the dataset
```{r readData}
# read the dataset
data_raw = read.table("aadt.txt",header=FALSE)
# check data by showing the first few entries of data
head(data_raw,5)
```

## Preprocess raw data
```{r preprocess_data}
# select only the columns with the predictor and response variable
data = data.frame(y=data_raw$V1,
                  x1=data_raw$V2,
                  x2=data_raw$V3,
                  x3=data_raw$V4,
                  x4=data_raw$V5)

#defining X4 to be 1 if there is access control and 0 if there isn't
data$x4[data$x4==1] <- 0
data$x4[data$x4==2] <- 1

# take a look at the modification to the data
head(data,5)
```

## Scatter Plot Matrix
```{r scatterPlot}
# plot the scatter plot matrix
pairs(data,panel=panel.smooth)
```
  
From the plot above, there exist relations between each predictor variables (X1 to X4) and the response variable (y). 

### Model: Additional e^x~2~^ term in the Multi Regression Model
From the plot above, there seemed to have an exponential relation between the response variable y and predictor variable x~2~. Hence, we fit the data with an additional e^x~2~^ term: $$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{3}X_{3}+\beta_{4}X_{4}+\beta_{5}e^{X_{2}}+\epsilon$$

This will be our suggested model 1.

### Fit Model 1
```{r fit1}
# fit a multiple linear regression model
mlr1 <- lm(y ~ x1+x2+x3+x4+I(exp(x2)), data=data)
summary(mlr1)
```

### Check the adequacy using t-value (standard error) and F-test between 2 models
From the result of model 1, predictor X~3~ is not significant due to very small t-value. Hence, it's coefficient may be equal to zero i.e $$H_{0}: \beta_{3} = 0$$

```{r model1}
# F-test between 2 models (full and model without predictor variable X3)
mlr1_alt <- lm(y ~ x1+x2+x4+I(exp(x2)), data=data)
# ANOVA
anova(mlr1_alt,mlr1)
```
From the above analysis, since the F-value is small, we cannot reject the null hypothesis at 0.1 level of significance, therefore, we eliminate the predictor variable X~3~ from further analysis

### Check normality of residuals
```{r model1_normality}
# normality checking
qqnorm(residuals(mlr1_alt), ylab='Residuals')
qqline(residuals(mlr1_alt))
```
  
From the QQ plot above, we can see that the residuals are not normally distributed. We will now draw some plots of the residuals against each predictor variable, X~1~, X~2~, X~4~  
```{r model1_resPlot}
# draw some plots of the residuals against each predictor variable
par(mfrow=c(1,4))
plot(fitted(mlr1_alt), residuals(mlr1_alt), ylab='Residuals', xlab='Fitted values')
plot(data$x1, residuals(mlr1_alt), ylab='Residuals', xlab='X1')
plot(data$x2, residuals(mlr1_alt), ylab='Residuals', xlab='X2')
plot(data$x4, residuals(mlr1_alt), ylab='Residuals', xlab='X4')
```
  
From the residuals against fitted values plot, we can observe that the variances of residuals have increased as the fitted values increases. From the residuals against X~1~ plot, there seemed to have a linear relationship between them. There isn't any obvious pattern seen from the plot of residuals against X~2~ and residuals against X~4~ plot. Therefore we can try to propose a model that includes the interaction between X~1~ and X~2~ as well as X~1~ and X~4~.
  
Hence, we fit the model:   $$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{4}X_{4}+\beta_{5}e^{X_{2}}+\beta_{6}X_{1}X_{2}+\beta_{7}X_{1}X_{4}+\epsilon$$

### Fit Model 2
```{r fit2}
# fit a multiple linear regression model
mlr2 <- lm(y ~ x1+x2+x4+I(exp(x2))+I(x1*x2)+I(x1*x4), data=data)
summary(mlr2)
```
### Check the adequacy using t-value (standard error) and F-test between 2 models
From the result of model 2, the term X~4~ is not significant due to very small t-value. Hence, it's coefficient may be equal to zero **after including interaction terms X~1~X~2~ and X~1~X~4~** i.e $$H_{0}: \beta_{4} = 0$$

```{r model2}
# F-test between 2 models (full and model without predictor variable X4)
mlr2_alt <- lm(y ~ x1+x2+I(exp(x2))+I(x1*x2)+I(x1*x4), data=data)
# ANOVA
anova(mlr2_alt,mlr2)
```
From the above analysis, since the F-value is small, we cannot reject the null hypothesis at 0.1 level of significance, therefore, we eliminate the predictor variable X~4~ from further analysis.

### Check normality of residuals
```{r model2_normality}
# normality checking
qqnorm(residuals(mlr2_alt), ylab='Residuals')
qqline(residuals(mlr2_alt))
```

From the QQ plot above, we can see that the residuals are not normally distributed, but it seems to be closer to normality as compared to the alternative model, model 1. Hence we will declare the alternative model of model 2 as model 3, with X~4~ term removed, to do further analysis. i.e. 
$$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{5}e^{X_{2}}+\beta_{6}X_{1}X_{2}+\beta_{7}X_{1}X_{4}+\epsilon$$

### Fit Model 3
```{r fit3}
# fit a multiple linear regression model
mlr3 <- lm(y ~ x1+x2+I(exp(x2))+I(x1*x2)+I(x1*x4), data=data)
summary(mlr3)
```

### Check the adequacy using t-value (standard error) and F-test between 2 models
From the result of model 3, it seems like the model fits quite well with reasonably large t-values and also the fact that the R-squared and adjusted R-squared value 0.9229 and 0.9195 respectively, which are reasonably high values. We will still try to remove one term with the lowest t-value among the terms, X~1~X~2~ to see if the model is a better fit to the data. i.e we test our hypothesis: $$H_{0}: \beta_{6} = 0$$

```{r model3}
# F-test between 3 models (full and model without predictor variable X1X2)
mlr3_alt <- lm(y ~ x1+x2+I(exp(x2))+I(x1*x4), data=data)
# ANOVA
anova(mlr3_alt,mlr3)
```
From the above analysis, since the F-value is large, we reject the null hypothesis at 0.1 level of significance, therefore, we **do not** eliminate the predictor variable X~1~X~2~ from further analysis.

### Final Model
After looking at the F-test, R-squared & adjusted R-squared as well as the significance of t-test, and also the comparison of the full and reduced model for model simplification, we proposed the following model to fit the data:  $$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{5}e^{X_{2}}+\beta_{6}X_{1}X_{2}+\beta_{7}X_{1}X_{4}+\epsilon$$

## Prediction
Using x1=50000, x2=3, x3=60 and x4=2 to fit our final model.
```{r prediction}
pred_data = data.frame(x1=50000, x2=3, x4=2)
# confidence interval of mean response
predict(mlr3,pred_data,interval='confidence', level=0.95)
# confidence interval of the predicted y value
predict(mlr3,pred_data,interval='prediction', level=0.95)
```